{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading input files\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "test_data = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ > Before Imputing \n",
      "PassengerId     0.000000\n",
      "Survived        0.000000\n",
      "Pclass          0.000000\n",
      "Name            0.000000\n",
      "Sex             0.000000\n",
      "Age            19.865320\n",
      "SibSp           0.000000\n",
      "Parch           0.000000\n",
      "Ticket          0.000000\n",
      "Fare            0.000000\n",
      "Cabin          77.104377\n",
      "Embarked        0.224467\n",
      "dtype: float64\n",
      "Age (mean) :  29.699\n",
      "Column have lot of Impurity / Missing Values so Dropped them :  Cabin\n",
      "Embarked (mode) :  S\n",
      "------ > After Imputing \n",
      "PassengerId    0.0\n",
      "Survived       0.0\n",
      "Pclass         0.0\n",
      "Name           0.0\n",
      "Sex            0.0\n",
      "Age            0.0\n",
      "SibSp          0.0\n",
      "Parch          0.0\n",
      "Ticket         0.0\n",
      "Fare           0.0\n",
      "Embarked       0.0\n",
      "dtype: float64\n",
      "------ > Before Imputing \n",
      "PassengerId     0.000000\n",
      "Pclass          0.000000\n",
      "Name            0.000000\n",
      "Sex             0.000000\n",
      "Age            20.574163\n",
      "SibSp           0.000000\n",
      "Parch           0.000000\n",
      "Ticket          0.000000\n",
      "Fare            0.239234\n",
      "Cabin          78.229665\n",
      "Embarked        0.000000\n",
      "dtype: float64\n",
      "Age (mean) :  30.273\n",
      "Fare (mean) :  35.627\n",
      "Column have lot of Impurity / Missing Values so Dropped them :  Cabin\n",
      "------ > After Imputing \n",
      "PassengerId    0.0\n",
      "Pclass         0.0\n",
      "Name           0.0\n",
      "Sex            0.0\n",
      "Age            0.0\n",
      "SibSp          0.0\n",
      "Parch          0.0\n",
      "Ticket         0.0\n",
      "Fare           0.0\n",
      "Embarked       0.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# =========================== Data Cleaning =============================== #\n",
    "#Null count in percentage\n",
    "def null_sum(x):\n",
    "    return (sum(x.isnull()) / len(x)) * 100\n",
    "\n",
    "def getImputeFree(dataset):\n",
    "    print(\"------ > Before Imputing \")\n",
    "    print(dataset.apply(null_sum, axis=0))                                             # Checking missing value count in percentage BEFORE performing imputting\n",
    "\n",
    "    for col in list(dataset.columns):\n",
    "        garbage_status = null_sum(dataset[col])                                        # getting missing value in percentage\n",
    "        if garbage_status > 0 and garbage_status <= 40:                                # replacing missing values which are less than 40% of data-set length\n",
    "            try:\n",
    "                dataset[col] = dataset[col].fillna(round(np.mean(dataset[col]),3))     # used mean as impute -> non-categorical data\n",
    "                print(col, \"(mean) : \",round(np.mean(dataset[col]),3))\n",
    "            except:\n",
    "                mode_class = np.array(train_data['Embarked'].value_counts().keys())[0] # getting mode of categorical data\n",
    "                dataset[col] = dataset[col].fillna(mode_class)                         # used mode as impute -> categorical data\n",
    "                print(col, \"(mode) : \", mode_class)\n",
    "        elif garbage_status > 40:                                                      # greater than 40 % missing values those columns are dropped\n",
    "            print(\"Column have lot of Impurity / Missing Values so Dropped them : \",col)\n",
    "            dataset.drop(col,axis=1,inplace=True)                                      # dropping the column\n",
    "\n",
    "    print(\"------ > After Imputing \")\n",
    "    print(dataset.apply(null_sum, axis=0))                                             # Checking missing value count in percentage AFTER performing imputing\n",
    "\n",
    "getImputeFree(train_data)\n",
    "getImputeFree(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================ Features creating using one-hot encoder============================ #\n",
    "train_predictors = train_data.drop(['PassengerId','Survived','Name','Ticket'], axis=1)   # dropped un-wanted cols and target column\n",
    "categorical_cols = [cname for cname in train_predictors.columns if train_predictors[cname].nunique() < 10 and train_predictors[cname].dtype == \"object\"]  # selected category col\n",
    "numeric_cols = [cname for cname in train_predictors.columns if train_predictors[cname].dtype in ['int64','float64']]  # selected numerical col\n",
    "my_cols = categorical_cols + numeric_cols  # selected col\n",
    "train_predictors = train_predictors[my_cols]\n",
    "encoded_train_predictors = pd.get_dummies(train_predictors)   # one-hot encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================== C4.5 Confusion Matrix =============================\n",
      "[[89 17]\n",
      " [28 45]]\n",
      "================================== C4.5 Accuracy Score ===============================\n",
      "0.7486033519553073\n"
     ]
    }
   ],
   "source": [
    "# ============================== #\n",
    "y_target = train_data['Survived'].values\n",
    "x_features = encoded_train_predictors.values\n",
    "\n",
    "# splitting train and validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_validate, y_train, y_validate = train_test_split(x_features, y_target, test_size=0.20, random_state=1)\n",
    "\n",
    "\n",
    "# model training\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf_model = DecisionTreeClassifier()\n",
    "clf_model.fit(x_train, y_train)\n",
    "\n",
    "# predicting\n",
    "validate_predict =  clf_model.predict(x_validate)\n",
    "\n",
    "# model validating\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(\"================================== C4.5 Confusion Matrix =============================\")\n",
    "print(confusion_matrix(y_validate, validate_predict))\n",
    "print(\"================================== C4.5 Accuracy Score ===============================\")\n",
    "print(accuracy_score(y_validate, validate_predict))\n",
    "\n",
    "\n",
    "test_predictors = test_data.drop(['PassengerId','Name','Ticket'], axis=1)\n",
    "test_x_featuers = pd.get_dummies(test_predictors).values\n",
    "test_predict = clf_model.predict(test_x_featuers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================== ID3 Confusion Matrix =============================\n",
      "[[89 17]\n",
      " [21 52]]\n",
      "================================== ID3 Accuracy Score ===============================\n",
      "0.7877094972067039\n"
     ]
    }
   ],
   "source": [
    "#===== ID3 =====#\n",
    "id3_clf_model = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "id3_clf_model.fit(x_train, y_train)\n",
    "\n",
    "# predicting\n",
    "validate_predict =  id3_clf_model.predict(x_validate)\n",
    "\n",
    "# model validating\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "print(\"================================== ID3 Confusion Matrix =============================\")\n",
    "print(confusion_matrix(y_validate, validate_predict))\n",
    "print(\"================================== ID3 Accuracy Score ===============================\")\n",
    "print(accuracy_score(y_validate, validate_predict))\n",
    "\n",
    "\n",
    "test_predictors = test_data.drop(['PassengerId','Name','Ticket'], axis=1)\n",
    "test_x_featuers = pd.get_dummies(test_predictors).values\n",
    "id3_test_predict = id3_clf_model.predict(test_x_featuers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
